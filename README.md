# Gradient Descent for Neural Networks (and Logistic Regression)

This repository demonstrates the **implementation of Gradient Descent from scratch** and its application to **Logistic Regression** and **Neural Networks** using Python.

The goal of this project is to deeply understand how optimization works **behind the scenes**, instead of relying only on high-level ML libraries.

---

## ðŸš€ What This Repository Covers

- Mathematical intuition of Gradient Descent
- Cost / Loss function formulation
- Partial derivatives and gradients
- Parameter updates step-by-step
- Logistic Regression trained using Gradient Descent
- Neural Network training using Gradient Descent
- Effect of learning rate on convergence
- Loss reduction visualization

---

## ðŸ§  Concepts Implemented

- Gradient Descent (Batch / Iterative)
- Logistic Regression from scratch
- Neural Network (Forward + Backward Propagation)
- Weight & bias optimization
- Loss minimization
- Model convergence analysis

---

## ðŸ›  Tech Stack

- **Python**
- **NumPy**
- **Matplotlib** (for visualization)
- (No ML libraries like `sklearn` used for training logic)

---

## ðŸ“‚ Project Structure

```text
.
â”œâ”€â”€ data/                 # Sample or generated datasets
â”œâ”€â”€ logistic_regression/  # Logistic Regression implementation
â”œâ”€â”€ neural_network/       # Neural Network implementation
â”œâ”€â”€ utils/                # Helper functions
â”œâ”€â”€ notebooks/            # Step-by-step explanations
â””â”€â”€ README.md
